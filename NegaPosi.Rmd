---
title: "Tweetsの時系列分析・他"
author: "石田基広"
date: "2021/12/13"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval=FALSE, cache = TRUE)
library(tidyverse)
```


# Tweetsデータ取得の準備


```{r}
library(tidyverse)
library(rtweet)
x <- search_tweets("自民党", n = 1)
x$text
```

## 言語指定でツィートを取得する方法


```{r}
tweets <- search_tweets("lang:jp", n = 1000, include_rts = FALSE)
```

```{r}
tweets %>% select(text)
```


```{r}
View(tweets)
```


## 画像を取り出す


```{r}
with_pictures <- search_tweets("#日本の絶景", n = 10,  include_rts = FALSE)
```

データのどこに画像情報があるのか？

```{r}
with_pictures %>% select(ext_media_url)  %>% unnest(cols = c(ext_media_url)) 
```


```{r}
pic_url <- with_pictures %>% select(ext_media_url)  %>% unnest(cols = c(ext_media_url)) 
```


```{r}
## 画像のurl ext_media_url を抽出
pic_url <- with_pictures %>% 
  ## あるツィートに画像のurlが複数ある場合がある
  unnest(cols = ext_media_url) %>% 
  ## もちろん、画像がないツィートもあるので削除
  filter(!is.na(ext_media_url)) %>%  #
  # 投稿日時と Tweets、投稿者、そして URLのみを抽出
  select(created_at, screen_name, text, ext_media_url)
```


```{r}
pic_url
```


画像を一気に保存するため、画像のURLを取り出す

```{r}
urls <- pic_url %>% select(ext_media_url) %>% pull()
```

```{r}
# 保存場所を指定
setwd("/home/ishida/tmp")
# setwd("C:/Users/Ishida/Downloads")
```

```{r}
# 保存するための関数
get_pictures <- function(url){
  id <- str_remove(url, "http://pbs.twimg.com/media/")
  download.file(url = url, destfile = id)
  ## サーバに対する礼儀
  Sys.sleep(1)
}
```

## Web Scraping

実行する

```{r}
urls %>% map(~ get_pictures(.x))
```


## ツィートからヒートマップ

毎日のツィート数を可視化

```{r}
## アカウント指定でツイートを収集
tw_data <- get_timeline("hirox246", n = 3200, include_rts = FALSE)
```
```{r}
dim(tw_data)
```


日付で投稿数を確認するのを目的とする。テキストそのものは使わない。日付列だけを残す。ただし、デフォルトだと世界標準時間(UTC)なので、日本標準時間(JST) に変換する。

```{r}
as.Date(as.POSIXct("2020-05-01 8:00:00")) # 2020-04-30 23:00 へ変換されてから、時間情報が削除されている
as.Date(as.POSIXct("2020-05-01 9:00:00")) # 2020-05-01 00:00 へ変換されてから、時間情報が削除されている

# UTC に統一して変換
td <- as.POSIXct("2020-05-01 8:00:00", tz = "UTC")
as.Date(td)
```

１行目のデータのタイムゾーンをJSTで表示
```{r}
tw_data %>% slice(1) %>% select (created_at) %>%
  # Twitter データのデフォルトはUTC
    mutate(created_at = as.POSIXct(x = created_at, tz="UTC")) %>%pull() %>% as.Date() 

```



すべての日付を変更し、別名で保存

```{r}
dates_tw <- tw_data %>% select (created_at) %>% mutate(created_at = as.POSIXct(x = created_at, tz="UTC")) %>% pull() %>% as.Date() 
```


```{r}
dates_tw %>% tail()
```

時間の設定


```{r}
# 日付時間を扱うパッケージ
library(lubridate)
tw_count <- dates_tw %>% 
  # 指定した時間単位で区切る
  floor_date(unit = "day") %>% 
  # Date型に変換
  as_date() %>% 
  # データフレームに変換
  tibble(terms = .) %>%
  # ツイート数をカウント
  count(terms) 

```

```{r}
tw_count %>% tail()
```

```{r}
NROW(tw_count)
```

毎日どれくらいつぶやいているかを確認するグラフを作成しますが、あるいは０回という日があるかもしれませんが、そういう日付は落ちています。例えば９月２４日にツィートが７個あり、９月２５日は０，そして９月２６日１１個という場合、データには４月２日を表す行がありません。

1 2020-09-24     7
2 2020-09-26    11
3 2020-09-27     2


そこで、いったん、取得したツィートの期間すべてを表す表を作成し、その表にツィート数を埋め込んでいくという処理を行います。
もしも、該当する日付がない場合は、０を埋め込みます。


```{r}
# ツイート期間を表すデータフレームを作成
term_df <- seq(
  # 一番古い日時
  floor_date(tail(dates_tw, 1), "day"),
  # 現在日時
  floor_date(now(), "day"), 
  by = "day"
) %>% 
  # 指定した期間刻みのベクトルを作成
  as_date() %>% 
  tibble(terms = .)

# 作成した表と集計結果を結合
tw_count2 <- left_join(term_df, tw_count, by = "terms") %>% 
  mutate(n = replace_na(n, 0)) # NAを0に置換

```

```{r}
# グラフのラベル用にデータフレームを整形
  tw_count2 <- tw_count2 %>% 
  # 年月を抽出
    mutate(year_mon = format(terms, "%Y-%m")) %>% 
  # 日を抽出
    mutate(day = format(terms, "%d")) 
```


```{r}
tw_count2 %>% ggplot(aes(x = year_mon, y = day, fill = n)) + 
    geom_tile() + 
  # ヒートマップ
    scale_fill_gradient(low = "white" , high = "#00A968") + 
  # 塗りつぶしの濃淡
    theme(axis.text.x = element_text(angle = 90, hjust = 1)) + # 軸目盛の傾き
    labs(title = paste0("@hirox246のツイート数"), 
         x = "year-mon", y = "day") 
```



## ネガポジ分析

ネガポジ判定に有効でない語や文字を省く

```{r}
# ツイートテキストの抽出と文字列処理
tw_text <- tw_data  %>% select(text) %>% pull() %>%  
  # リプライの除去
  str_remove_all("^@.*?\\s") %>% 
  # urlの除去
  str_remove_all(("https?://[\\w/:%#\\$&\\?\\(\\)~\\.=\\+\\-]+")) %>% 
  # # 絵文字の除去（
  str_remove_all("[\U0001F004-\U000207BF]") 
```


```{r}
# 期間ごとにまとめる
text_df <- data.frame(
  # 日付で区切る
  terms = as.Date(floor_date(dates_tw, "day")), 
  texts = tw_text
) %>% 
  group_by(terms) %>% 
  # 同じ日付のツィートを1つにまとめる
  summarise(texts = paste(texts, collapse = "\n")) 
```

```{r}
text_df %>% slice(1) %>% select(texts) %>% pull()
```

```{r}
text_df %>% colnames()
```

日付ごとに形態素解析にかける
```{r}
library(RMeCab)
rmecabrc <- function(term, text){
  x <- unlist(RMeCabC(text, 1))
  tibble(Day = term, TERM= x)
}
```

```{r}
day_df <- map2_dfr(text_df$terms, text_df$texts,
                   ~rmecabrc(..1, ..2))
```


```{r}
day_df %>% head()
```

```{r}
# 単語感情極性対応表の取得
dic <- read.table(
  "http://www.lr.pi.titech.ac.jp/~takamura/pubs/pn_ja.dic", 
  sep = ":", stringsAsFactors = FALSE,fileEncoding = "CP932"
)

```

```{r}
dic %>% NROW()
```


```{r}
dic %>% filter(V1 == "大人")
```

```{r}
## 同じ単語が複数ある場合は平均値
dic <- dic %>% arrange(V1) %>% group_by(V1) %>% summarise(SCORE = mean(V4)) %>% select(TERM = V1, SCORE)
dic %>% head()
```



```{r}
day_df <- day_df %>% left_join(dic)
```

```{r}
day_df2 <- day_df %>% group_by(Day) %>% summarise(NP = sum(SCORE, na.rm = TRUE))
```



```{r}
day_df2 %>% head()
```

```{r}
day_df2 %>% ggplot(aes(x=Day, y = NP)) + geom_line()+ 
  # 折れ線グラフ
  scale_x_date(date_breaks = "2 weeks") + # x軸目盛(day)
  theme(axis.text.x = element_text(angle = 90)) # 
```













```{r}
# 一時テキストファイルの保存先を指定
tmp_folder <- "/home/ishida/tmp"
# 分析結果の受け皿を初期化
score_df <- data.frame()
for(i in 1:nrow(text_df)) {
  # 一時ファイルパスを作成
  tmp_file_name <- paste("hirox246_", text_df[["terms"]][i], ".txt", sep = "")
  tmp_path <- paste(tmp_folder, tmp_file_name, sep = "/")
  # テキストファイルを書き出し
  write(text_df[["texts"]][i], tmp_path)
}

  
```






```{r}
# ネガポジ辞書の作成
np_dic <- np_dic_original %>% 
  select(TERM = V1, POS1 = V3, allocation = V4) %>% # 列の選択
  distinct(TERM, .keep_all = TRUE) # 重複の除去
head(np_dic)
```


```{r}
# MeCabによる形態素解析
library(RMeCab)
mecab_df <- docDF(tmp_folder, type = 1, pos = c("動詞", "形容詞", "副詞", "助動詞"))
```

```{r}
## MeCab の解析結果と、ネガポジ辞書を照合
tmp_score_df <- mecab_df %>% 
      left_join(np_dic, by = c("TERM", "POS1")) %>%
  # 各単語に配点を付与
      mutate(allocation = replace_na(allocation, 0))
```


```{r}
tmp_score_df <- tmp_score_df %>% group_by(terms) %>% 
  summarize() %>% 
      # 日付の追加
      mutate(allocation = replace_na(allocation, 0)) %>% 
      # 配点がNAの場合0に
      mutate(score = allocation * FREQ) %>% 
      # (スコア) = (配点) * (語数)
      mutate(
        np_label = case_when(
          score < 0 ~ "negative", # (スコア) < 0ならネガ
          score > 0 ~ "positive", # (スコア) > 0ならポジ
          score == 0 ~ "neutral"  # (スコア) = 0ならニュート
        )
      ) # ネガポジラベルを付与
    
# 全期間の結果を結合
score_df <- rbind(score_df, tmp_score_df)
```


```{r}
# 期間ごとにネガスコア・ポジスコアの合計
result_df <- score_df %>% 
  select(terms, np_label, score, FREQ) %>% 
  group_by(terms, np_label) %>% 
  summarise(score = sum(score), FREQ = sum(FREQ)) # スコアと頻度を合算

```

```{r}
result_df
```

```{r}
# ネガポジ推移
ggplot(result_df, aes(x = terms, y = score)) + 
  geom_bar(mapping = aes(fill = np_label, color = np_label), stat = "identity") + # 棒グラフ
  scale_fill_manual(values = c("#00A968", "yellow", "orange")) + # 塗りつぶしの色
  scale_color_manual(values = c("#00A968", "yellow", "orange")) + # 枠の色
  geom_line(stat = "summary", fun = "sum", color = "blue") + # 折れ線グラフ
  scale_x_date(date_breaks = "2 weeks") + # x軸目盛(day)
  #scale_x_date(date_breaks = "1 month", date_labels = "%Y-%m") + # x軸目盛(mon)
  theme(axis.text.x = element_text(angle = 90)) + # x軸目盛の傾き
  labs(title = paste0("@hirox246のネガポジ推移"), 
       x = "day") # ラベル

```

